{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = 'dataset/abalone.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3       4       5       6      7   8\n",
       "0  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150  15\n",
       "1  M  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070   7\n",
       "2  F  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210   9\n",
       "3  M  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155  10\n",
       "4  I  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055   7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(fpath, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>shucked_weight</th>\n",
       "      <th>viscera_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   shell_weight  rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'rings']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 9)\n",
      "            length     diameter       height  whole_weight  shucked_weight  \\\n",
      "count  4177.000000  4177.000000  4177.000000   4177.000000     4177.000000   \n",
      "mean      0.523992     0.407881     0.139516      0.828742        0.359367   \n",
      "std       0.120093     0.099240     0.041827      0.490389        0.221963   \n",
      "min       0.075000     0.055000     0.000000      0.002000        0.001000   \n",
      "25%       0.450000     0.350000     0.115000      0.441500        0.186000   \n",
      "50%       0.545000     0.425000     0.140000      0.799500        0.336000   \n",
      "75%       0.615000     0.480000     0.165000      1.153000        0.502000   \n",
      "max       0.815000     0.650000     1.130000      2.825500        1.488000   \n",
      "\n",
      "       viscera_weight  shell_weight        rings  \n",
      "count     4177.000000   4177.000000  4177.000000  \n",
      "mean         0.180594      0.238831     9.933684  \n",
      "std          0.109614      0.139203     3.224169  \n",
      "min          0.000500      0.001500     1.000000  \n",
      "25%          0.093500      0.130000     8.000000  \n",
      "50%          0.171000      0.234000     9.000000  \n",
      "75%          0.253000      0.329000    11.000000  \n",
      "max          0.760000      1.005000    29.000000  \n",
      "sex                object\n",
      "length            float64\n",
      "diameter          float64\n",
      "height            float64\n",
      "whole_weight      float64\n",
      "shucked_weight    float64\n",
      "viscera_weight    float64\n",
      "shell_weight      float64\n",
      "rings               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.describe())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex               0\n",
       "length            0\n",
       "diameter          0\n",
       "height            0\n",
       "whole_weight      0\n",
       "shucked_weight    0\n",
       "viscera_weight    0\n",
       "shell_weight      0\n",
       "rings             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x104858250>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.drop('rings', axis=1).boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'rings'\n",
    "features = list(df.columns)\n",
    "features.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capping outliers\n",
    "for f in features[1:]:\n",
    "    upper_lim = df[f].quantile(0.75) + ((df[f].quantile(0.75) - df[f].quantile(0.25)) * 1.5)\n",
    "    lower_lim = df[f].quantile(0.25) - ((df[f].quantile(0.75) - df[f].quantile(0.25)) * 1.5)\n",
    "    \n",
    "    df.loc[(df[f] > upper_lim), f] = upper_lim\n",
    "    df.loc[(df[f] < lower_lim), f] = lower_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df[features], hue='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(df[features].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[target]\n",
    "X = df[features]\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "train_X = scaler.transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.084307453282412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(random_state=1)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.820380374088831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=1)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.139220231158612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR()\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0956486020872838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.097858471240914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(random_state=1)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4175395772676613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9886015169424718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "model = LinearRegression()\n",
    "poly = PolynomialFeatures(2)\n",
    "X_train_transform = poly.fit_transform(train_X)\n",
    "X_test_transform = poly.fit_transform(test_X)\n",
    "model.fit(X_train_transform, train_y)\n",
    "\n",
    "pred = model.predict(X_test_transform)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0925731947698942\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(objective ='reg:squarederror', learning_rate = 0.1)\n",
    "poly = PolynomialFeatures(2)\n",
    "X_train_transform = poly.fit_transform(train_X)\n",
    "X_test_transform = poly.fit_transform(test_X)\n",
    "model.fit(X_train_transform, train_y)\n",
    "\n",
    "pred = model.predict(X_test_transform)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.layers as layers\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)])\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 17,345\n",
      "Trainable params: 17,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3007 samples, validate on 752 samples\n",
      "Epoch 1/500\n",
      "3007/3007 [==============================] - 1s 182us/step - loss: 103.3180 - val_loss: 88.3081\n",
      "Epoch 2/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 67.4937 - val_loss: 42.3095\n",
      "Epoch 3/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 27.1475 - val_loss: 19.6804\n",
      "Epoch 4/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 17.0704 - val_loss: 14.1488\n",
      "Epoch 5/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 12.4644 - val_loss: 10.3737\n",
      "Epoch 6/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 9.8252 - val_loss: 8.0213\n",
      "Epoch 7/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 8.3526 - val_loss: 6.6389\n",
      "Epoch 8/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 7.1235 - val_loss: 5.9544\n",
      "Epoch 9/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 6.5366 - val_loss: 5.5335\n",
      "Epoch 10/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 6.3906 - val_loss: 5.3106\n",
      "Epoch 11/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 6.1247 - val_loss: 5.1759\n",
      "Epoch 12/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 6.0704 - val_loss: 5.0684\n",
      "Epoch 13/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 5.8305 - val_loss: 5.0185\n",
      "Epoch 14/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 5.7374 - val_loss: 4.9346\n",
      "Epoch 15/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 5.8191 - val_loss: 4.8741\n",
      "Epoch 16/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 5.6692 - val_loss: 4.8240\n",
      "Epoch 17/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 5.4882 - val_loss: 4.8036\n",
      "Epoch 18/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 5.4912 - val_loss: 4.7638\n",
      "Epoch 19/500\n",
      "3007/3007 [==============================] - 0s 59us/step - loss: 5.5263 - val_loss: 4.7279\n",
      "Epoch 20/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 5.5603 - val_loss: 4.6743\n",
      "Epoch 21/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 5.4328 - val_loss: 4.6794\n",
      "Epoch 22/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 5.4770 - val_loss: 4.6216\n",
      "Epoch 23/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 5.4183 - val_loss: 4.6261\n",
      "Epoch 24/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 5.3722 - val_loss: 4.5795\n",
      "Epoch 25/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 5.5151 - val_loss: 4.5651\n",
      "Epoch 26/500\n",
      "3007/3007 [==============================] - 0s 56us/step - loss: 5.2482 - val_loss: 4.5518\n",
      "Epoch 27/500\n",
      "3007/3007 [==============================] - 0s 53us/step - loss: 5.3792 - val_loss: 4.5378\n",
      "Epoch 28/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 5.2951 - val_loss: 4.5139\n",
      "Epoch 29/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 5.0960 - val_loss: 4.5454\n",
      "Epoch 30/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 5.2629 - val_loss: 4.5019\n",
      "Epoch 31/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 5.2588 - val_loss: 4.4545\n",
      "Epoch 32/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 5.2764 - val_loss: 4.4394\n",
      "Epoch 33/500\n",
      "3007/3007 [==============================] - 0s 39us/step - loss: 5.2480 - val_loss: 4.4342\n",
      "Epoch 34/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 5.3283 - val_loss: 4.4567\n",
      "Epoch 35/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 5.2866 - val_loss: 4.4342\n",
      "Epoch 36/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 5.1409 - val_loss: 4.4423\n",
      "Epoch 37/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 5.2457 - val_loss: 4.3893\n",
      "Epoch 38/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 5.3079 - val_loss: 4.3864\n",
      "Epoch 39/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 5.1857 - val_loss: 4.4093\n",
      "Epoch 40/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 5.2189 - val_loss: 4.4269\n",
      "Epoch 41/500\n",
      "3007/3007 [==============================] - 0s 58us/step - loss: 5.2382 - val_loss: 4.3909\n",
      "Epoch 42/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 5.1965 - val_loss: 4.3631\n",
      "Epoch 43/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 5.1780 - val_loss: 4.3620\n",
      "Epoch 44/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 5.0422 - val_loss: 4.3859\n",
      "Epoch 45/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 5.0885 - val_loss: 4.3415\n",
      "Epoch 46/500\n",
      "3007/3007 [==============================] - 0s 58us/step - loss: 5.0909 - val_loss: 4.3470\n",
      "Epoch 47/500\n",
      "3007/3007 [==============================] - 0s 67us/step - loss: 5.0442 - val_loss: 4.3275\n",
      "Epoch 48/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 5.2093 - val_loss: 4.3234\n",
      "Epoch 49/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 5.0588 - val_loss: 4.3253\n",
      "Epoch 50/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 5.0780 - val_loss: 4.3241\n",
      "Epoch 51/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 4.9911 - val_loss: 4.2643\n",
      "Epoch 52/500\n",
      "3007/3007 [==============================] - 0s 39us/step - loss: 5.1934 - val_loss: 4.2855\n",
      "Epoch 53/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 5.0857 - val_loss: 4.2619\n",
      "Epoch 54/500\n",
      "3007/3007 [==============================] - 0s 57us/step - loss: 5.0451 - val_loss: 4.3189\n",
      "Epoch 55/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 5.1240 - val_loss: 4.2888\n",
      "Epoch 56/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 5.0033 - val_loss: 4.3031\n",
      "Epoch 57/500\n",
      "3007/3007 [==============================] - 0s 36us/step - loss: 5.0001 - val_loss: 4.2622\n",
      "Epoch 58/500\n",
      "3007/3007 [==============================] - 0s 36us/step - loss: 5.1604 - val_loss: 4.2577\n",
      "Epoch 59/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 5.0171 - val_loss: 4.2353\n",
      "Epoch 60/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.9900 - val_loss: 4.2744\n",
      "Epoch 61/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 5.1072 - val_loss: 4.2667\n",
      "Epoch 62/500\n",
      "3007/3007 [==============================] - 0s 53us/step - loss: 5.0310 - val_loss: 4.2254\n",
      "Epoch 63/500\n",
      "3007/3007 [==============================] - 0s 58us/step - loss: 4.9146 - val_loss: 4.2232\n",
      "Epoch 64/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 5.0215 - val_loss: 4.2956\n",
      "Epoch 65/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 4.9928 - val_loss: 4.2362\n",
      "Epoch 66/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 4.9969 - val_loss: 4.2423\n",
      "Epoch 67/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 5.0324 - val_loss: 4.2243\n",
      "Epoch 68/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 4.8536 - val_loss: 4.2152\n",
      "Epoch 69/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 4.9541 - val_loss: 4.1901\n",
      "Epoch 70/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.9638 - val_loss: 4.2409\n",
      "Epoch 71/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 4.9874 - val_loss: 4.1719\n",
      "Epoch 72/500\n",
      "3007/3007 [==============================] - 0s 39us/step - loss: 4.8703 - val_loss: 4.1977\n",
      "Epoch 73/500\n",
      "3007/3007 [==============================] - 0s 56us/step - loss: 4.9844 - val_loss: 4.2021\n",
      "Epoch 74/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.8733 - val_loss: 4.1737\n",
      "Epoch 75/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 5.0381 - val_loss: 4.1678\n",
      "Epoch 76/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 4.8065 - val_loss: 4.1688\n",
      "Epoch 77/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 4.9038 - val_loss: 4.1678\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3007/3007 [==============================] - 0s 40us/step - loss: 4.8708 - val_loss: 4.1869\n",
      "Epoch 79/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.8283 - val_loss: 4.1650\n",
      "Epoch 80/500\n",
      "3007/3007 [==============================] - 0s 58us/step - loss: 4.9087 - val_loss: 4.1433\n",
      "Epoch 81/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.9004 - val_loss: 4.1713\n",
      "Epoch 82/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 4.9001 - val_loss: 4.1503\n",
      "Epoch 83/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 4.8238 - val_loss: 4.1459\n",
      "Epoch 84/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 4.9481 - val_loss: 4.1648\n",
      "Epoch 85/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 4.9457 - val_loss: 4.1230\n",
      "Epoch 86/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.8236 - val_loss: 4.1113\n",
      "Epoch 87/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.7861 - val_loss: 4.1228\n",
      "Epoch 88/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.8213 - val_loss: 4.2022\n",
      "Epoch 89/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.8902 - val_loss: 4.0903\n",
      "Epoch 90/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.8566 - val_loss: 4.1194\n",
      "Epoch 91/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.7851 - val_loss: 4.1512\n",
      "Epoch 92/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 4.9248 - val_loss: 4.1447\n",
      "Epoch 93/500\n",
      "3007/3007 [==============================] - 0s 37us/step - loss: 4.9167 - val_loss: 4.1202\n",
      "Epoch 94/500\n",
      "3007/3007 [==============================] - 0s 38us/step - loss: 4.7972 - val_loss: 4.1308\n",
      "Epoch 95/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.8532 - val_loss: 4.1153\n",
      "Epoch 96/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.8165 - val_loss: 4.1077\n",
      "Epoch 97/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.8376 - val_loss: 4.0781\n",
      "Epoch 98/500\n",
      "3007/3007 [==============================] - 0s 50us/step - loss: 4.8089 - val_loss: 4.1026\n",
      "Epoch 99/500\n",
      "3007/3007 [==============================] - 0s 50us/step - loss: 4.8878 - val_loss: 4.1143\n",
      "Epoch 100/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.8850 - val_loss: 4.0808\n",
      "Epoch 101/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.8476 - val_loss: 4.0942\n",
      "Epoch 102/500\n",
      "3007/3007 [==============================] - 0s 50us/step - loss: 4.7930 - val_loss: 4.0987\n",
      "Epoch 103/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.8272 - val_loss: 4.1029\n",
      "Epoch 104/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.8075 - val_loss: 4.0836\n",
      "Epoch 105/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.8162 - val_loss: 4.1247\n",
      "Epoch 106/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.7116 - val_loss: 4.0963\n",
      "Epoch 107/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.7550 - val_loss: 4.2011\n",
      "Epoch 108/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.8183 - val_loss: 4.0590\n",
      "Epoch 109/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.8407 - val_loss: 4.0749\n",
      "Epoch 110/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.8238 - val_loss: 4.0489\n",
      "Epoch 111/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.8737 - val_loss: 4.0461\n",
      "Epoch 112/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.7289 - val_loss: 4.0955\n",
      "Epoch 113/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.7970 - val_loss: 4.0734\n",
      "Epoch 114/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.8967 - val_loss: 4.0898\n",
      "Epoch 115/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.7453 - val_loss: 4.0777\n",
      "Epoch 116/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.7174 - val_loss: 4.0648\n",
      "Epoch 117/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.6949 - val_loss: 4.0538\n",
      "Epoch 118/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.8958 - val_loss: 4.0725\n",
      "Epoch 119/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.8465 - val_loss: 4.0689\n",
      "Epoch 120/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.8471 - val_loss: 4.0485\n",
      "Epoch 121/500\n",
      "3007/3007 [==============================] - 0s 53us/step - loss: 4.7247 - val_loss: 4.0451\n",
      "Epoch 122/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.8362 - val_loss: 4.0616\n",
      "Epoch 123/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.6801 - val_loss: 4.0778\n",
      "Epoch 124/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.7041 - val_loss: 4.0367\n",
      "Epoch 125/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.7835 - val_loss: 4.0517\n",
      "Epoch 126/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.7493 - val_loss: 4.0156\n",
      "Epoch 127/500\n",
      "3007/3007 [==============================] - 0s 53us/step - loss: 4.5942 - val_loss: 4.0582\n",
      "Epoch 128/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.7549 - val_loss: 4.0365\n",
      "Epoch 129/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.7772 - val_loss: 4.0257\n",
      "Epoch 130/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.7551 - val_loss: 4.0315\n",
      "Epoch 131/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.7684 - val_loss: 3.9979\n",
      "Epoch 132/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.7708 - val_loss: 4.0430\n",
      "Epoch 133/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.7969 - val_loss: 4.0169\n",
      "Epoch 134/500\n",
      "3007/3007 [==============================] - 0s 53us/step - loss: 4.6990 - val_loss: 4.0074\n",
      "Epoch 135/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.7673 - val_loss: 4.0154\n",
      "Epoch 136/500\n",
      "3007/3007 [==============================] - 0s 61us/step - loss: 4.7335 - val_loss: 4.0074\n",
      "Epoch 137/500\n",
      "3007/3007 [==============================] - 0s 61us/step - loss: 4.7478 - val_loss: 4.0184\n",
      "Epoch 138/500\n",
      "3007/3007 [==============================] - 0s 58us/step - loss: 4.7235 - val_loss: 4.0007\n",
      "Epoch 139/500\n",
      "3007/3007 [==============================] - 0s 58us/step - loss: 4.7762 - val_loss: 4.0173\n",
      "Epoch 140/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.6497 - val_loss: 4.0613\n",
      "Epoch 141/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.7306 - val_loss: 4.0003\n",
      "Epoch 142/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.5809 - val_loss: 4.0087\n",
      "Epoch 143/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.6196 - val_loss: 3.9784\n",
      "Epoch 144/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.6371 - val_loss: 4.0033\n",
      "Epoch 145/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.6485 - val_loss: 3.9907\n",
      "Epoch 146/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.6958 - val_loss: 4.0013\n",
      "Epoch 147/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.6766 - val_loss: 4.0028\n",
      "Epoch 148/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.6581 - val_loss: 3.9833\n",
      "Epoch 149/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.8048 - val_loss: 3.9768\n",
      "Epoch 150/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.6474 - val_loss: 3.9777\n",
      "Epoch 151/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.7295 - val_loss: 3.9774\n",
      "Epoch 152/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.6862 - val_loss: 3.9933\n",
      "Epoch 153/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.6809 - val_loss: 3.9965\n",
      "Epoch 154/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.6071 - val_loss: 3.9750\n",
      "Epoch 155/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.7064 - val_loss: 3.9785\n",
      "Epoch 156/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.7900 - val_loss: 3.9651\n",
      "Epoch 157/500\n",
      "3007/3007 [==============================] - 0s 67us/step - loss: 4.7069 - val_loss: 3.9746\n",
      "Epoch 158/500\n",
      "3007/3007 [==============================] - 0s 57us/step - loss: 4.6396 - val_loss: 3.9857\n",
      "Epoch 159/500\n",
      "3007/3007 [==============================] - 0s 56us/step - loss: 4.6873 - val_loss: 3.9731\n",
      "Epoch 160/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.6832 - val_loss: 3.9872\n",
      "Epoch 161/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.6794 - val_loss: 3.9898\n",
      "Epoch 162/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.7666 - val_loss: 3.9850\n",
      "Epoch 163/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 4.6872 - val_loss: 3.9617\n",
      "Epoch 164/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.6374 - val_loss: 3.9736\n",
      "Epoch 165/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.6292 - val_loss: 3.9484\n",
      "Epoch 166/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 4.6832 - val_loss: 3.9620\n",
      "Epoch 167/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 4.6458 - val_loss: 3.9690\n",
      "Epoch 168/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 4.6317 - val_loss: 4.0069\n",
      "Epoch 169/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.6502 - val_loss: 3.9648\n",
      "Epoch 170/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.6702 - val_loss: 3.9610\n",
      "Epoch 171/500\n",
      "3007/3007 [==============================] - 0s 40us/step - loss: 4.7481 - val_loss: 3.9712\n",
      "Epoch 172/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.6764 - val_loss: 3.9881\n",
      "Epoch 173/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.6789 - val_loss: 3.9764\n",
      "Epoch 174/500\n",
      "3007/3007 [==============================] - 0s 59us/step - loss: 4.6218 - val_loss: 3.9886\n",
      "Epoch 175/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.7368 - val_loss: 3.9483\n",
      "Epoch 176/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.6436 - val_loss: 3.9503\n",
      "Epoch 177/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.6307 - val_loss: 3.9680\n",
      "Epoch 178/500\n",
      "3007/3007 [==============================] - 0s 41us/step - loss: 4.7295 - val_loss: 3.9619\n",
      "Epoch 179/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.6106 - val_loss: 3.9599\n",
      "Epoch 180/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.5967 - val_loss: 3.9808\n",
      "Epoch 181/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.6951 - val_loss: 3.9996\n",
      "Epoch 182/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.7211 - val_loss: 3.9891\n",
      "Epoch 183/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.5729 - val_loss: 3.9684\n",
      "Epoch 184/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.5608 - val_loss: 3.9345\n",
      "Epoch 185/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.6145 - val_loss: 3.9489\n",
      "Epoch 186/500\n",
      "3007/3007 [==============================] - 0s 56us/step - loss: 4.6527 - val_loss: 3.9369\n",
      "Epoch 187/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.6401 - val_loss: 3.9590\n",
      "Epoch 188/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.4639 - val_loss: 3.9397\n",
      "Epoch 189/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.6170 - val_loss: 3.9484\n",
      "Epoch 190/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.5325 - val_loss: 3.9427\n",
      "Epoch 191/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.6223 - val_loss: 3.9370\n",
      "Epoch 192/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.5946 - val_loss: 3.9923\n",
      "Epoch 193/500\n",
      "3007/3007 [==============================] - 0s 63us/step - loss: 4.7160 - val_loss: 3.9597\n",
      "Epoch 194/500\n",
      "3007/3007 [==============================] - 0s 60us/step - loss: 4.5950 - val_loss: 3.9547\n",
      "Epoch 195/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5843 - val_loss: 3.9604\n",
      "Epoch 196/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.5857 - val_loss: 3.9536\n",
      "Epoch 197/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.6074 - val_loss: 3.9529\n",
      "Epoch 198/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.5610 - val_loss: 3.9438\n",
      "Epoch 199/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.5661 - val_loss: 3.9543\n",
      "Epoch 200/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.6236 - val_loss: 3.9408\n",
      "Epoch 201/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.6847 - val_loss: 3.9357\n",
      "Epoch 202/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.6591 - val_loss: 3.9474\n",
      "Epoch 203/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.5633 - val_loss: 3.9449\n",
      "Epoch 204/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.5930 - val_loss: 3.9513\n",
      "Epoch 205/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.5720 - val_loss: 3.9761\n",
      "Epoch 206/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.5615 - val_loss: 3.9628\n",
      "Epoch 207/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5461 - val_loss: 3.9571\n",
      "Epoch 208/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.5497 - val_loss: 3.9372\n",
      "Epoch 209/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.5131 - val_loss: 3.9659\n",
      "Epoch 210/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.5536 - val_loss: 3.9394\n",
      "Epoch 211/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.5213 - val_loss: 3.9265\n",
      "Epoch 212/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.6117 - val_loss: 3.9658\n",
      "Epoch 213/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.5649 - val_loss: 3.9456\n",
      "Epoch 214/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.5640 - val_loss: 3.9634\n",
      "Epoch 215/500\n",
      "3007/3007 [==============================] - 0s 58us/step - loss: 4.5586 - val_loss: 3.9519\n",
      "Epoch 216/500\n",
      "3007/3007 [==============================] - 0s 70us/step - loss: 4.5572 - val_loss: 3.9414\n",
      "Epoch 217/500\n",
      "3007/3007 [==============================] - 0s 58us/step - loss: 4.5043 - val_loss: 3.9707\n",
      "Epoch 218/500\n",
      "3007/3007 [==============================] - 0s 53us/step - loss: 4.6375 - val_loss: 3.9411\n",
      "Epoch 219/500\n",
      "3007/3007 [==============================] - 0s 56us/step - loss: 4.5522 - val_loss: 3.9500\n",
      "Epoch 220/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.5359 - val_loss: 3.9669\n",
      "Epoch 221/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5619 - val_loss: 3.9492\n",
      "Epoch 222/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.5989 - val_loss: 3.9099\n",
      "Epoch 223/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.6181 - val_loss: 3.9661\n",
      "Epoch 224/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5606 - val_loss: 4.0012\n",
      "Epoch 225/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5541 - val_loss: 3.9358\n",
      "Epoch 226/500\n",
      "3007/3007 [==============================] - 0s 62us/step - loss: 4.6652 - val_loss: 3.9319\n",
      "Epoch 227/500\n",
      "3007/3007 [==============================] - 0s 61us/step - loss: 4.5197 - val_loss: 3.9272\n",
      "Epoch 228/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.5320 - val_loss: 3.9646\n",
      "Epoch 229/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5802 - val_loss: 3.9399\n",
      "Epoch 230/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.5224 - val_loss: 3.9425\n",
      "Epoch 231/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.5175 - val_loss: 3.9297\n",
      "Epoch 232/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3007/3007 [==============================] - 0s 50us/step - loss: 4.5487 - val_loss: 3.9438\n",
      "Epoch 233/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.6120 - val_loss: 3.9118\n",
      "Epoch 234/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.6385 - val_loss: 3.9336\n",
      "Epoch 235/500\n",
      "3007/3007 [==============================] - 0s 64us/step - loss: 4.5180 - val_loss: 3.9651\n",
      "Epoch 236/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.5372 - val_loss: 3.9612\n",
      "Epoch 237/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.6467 - val_loss: 3.9447\n",
      "Epoch 238/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.5559 - val_loss: 3.9425\n",
      "Epoch 239/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.5478 - val_loss: 3.9454\n",
      "Epoch 240/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.5592 - val_loss: 3.9168\n",
      "Epoch 241/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4875 - val_loss: 3.9544\n",
      "Epoch 242/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.5420 - val_loss: 3.9257\n",
      "Epoch 243/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.5586 - val_loss: 3.9669\n",
      "Epoch 244/500\n",
      "3007/3007 [==============================] - 0s 53us/step - loss: 4.5735 - val_loss: 3.9072\n",
      "Epoch 245/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.5503 - val_loss: 3.9288\n",
      "Epoch 246/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5554 - val_loss: 3.9543\n",
      "Epoch 247/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.5709 - val_loss: 3.9526\n",
      "Epoch 248/500\n",
      "3007/3007 [==============================] - 0s 56us/step - loss: 4.4553 - val_loss: 3.9531\n",
      "Epoch 249/500\n",
      "3007/3007 [==============================] - 0s 50us/step - loss: 4.5311 - val_loss: 3.9202\n",
      "Epoch 250/500\n",
      "3007/3007 [==============================] - 0s 52us/step - loss: 4.4974 - val_loss: 3.9495\n",
      "Epoch 251/500\n",
      "3007/3007 [==============================] - 0s 50us/step - loss: 4.5745 - val_loss: 3.9085\n",
      "Epoch 252/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.6116 - val_loss: 3.9188\n",
      "Epoch 253/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.5133 - val_loss: 3.9167\n",
      "Epoch 254/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5232 - val_loss: 3.9070\n",
      "Epoch 255/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4824 - val_loss: 3.9219\n",
      "Epoch 256/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.4994 - val_loss: 3.9107\n",
      "Epoch 257/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.4567 - val_loss: 3.9037\n",
      "Epoch 258/500\n",
      "3007/3007 [==============================] - 0s 55us/step - loss: 4.5486 - val_loss: 3.9380\n",
      "Epoch 259/500\n",
      "3007/3007 [==============================] - 0s 56us/step - loss: 4.4398 - val_loss: 3.9208\n",
      "Epoch 260/500\n",
      "3007/3007 [==============================] - 0s 56us/step - loss: 4.5419 - val_loss: 3.9143\n",
      "Epoch 261/500\n",
      "3007/3007 [==============================] - 0s 54us/step - loss: 4.5894 - val_loss: 3.9246\n",
      "Epoch 262/500\n",
      "3007/3007 [==============================] - 0s 58us/step - loss: 4.5262 - val_loss: 3.9076\n",
      "Epoch 263/500\n",
      "3007/3007 [==============================] - 0s 51us/step - loss: 4.4630 - val_loss: 3.9497\n",
      "Epoch 264/500\n",
      "3007/3007 [==============================] - 0s 53us/step - loss: 4.5368 - val_loss: 3.9086\n",
      "Epoch 265/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.5571 - val_loss: 3.9174\n",
      "Epoch 266/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.4060 - val_loss: 3.9277\n",
      "Epoch 267/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.5639 - val_loss: 3.9061\n",
      "Epoch 268/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5349 - val_loss: 3.9237\n",
      "Epoch 269/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4436 - val_loss: 3.9526\n",
      "Epoch 270/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4710 - val_loss: 3.9150\n",
      "Epoch 271/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4596 - val_loss: 3.9297\n",
      "Epoch 272/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4924 - val_loss: 3.9315\n",
      "Epoch 273/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4682 - val_loss: 3.9090\n",
      "Epoch 274/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.4476 - val_loss: 3.9081\n",
      "Epoch 275/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4843 - val_loss: 3.9350\n",
      "Epoch 276/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.4540 - val_loss: 3.8972\n",
      "Epoch 277/500\n",
      "3007/3007 [==============================] - 0s 59us/step - loss: 4.4607 - val_loss: 3.9808\n",
      "Epoch 278/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.4704 - val_loss: 3.9133\n",
      "Epoch 279/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.4591 - val_loss: 3.9191\n",
      "Epoch 280/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.5336 - val_loss: 3.9228\n",
      "Epoch 281/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4339 - val_loss: 3.9029\n",
      "Epoch 282/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4996 - val_loss: 3.8961\n",
      "Epoch 283/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4515 - val_loss: 3.9129\n",
      "Epoch 284/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.5492 - val_loss: 3.9402\n",
      "Epoch 285/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.4771 - val_loss: 3.9037\n",
      "Epoch 286/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.5164 - val_loss: 3.9178\n",
      "Epoch 287/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3594 - val_loss: 3.9278\n",
      "Epoch 288/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4430 - val_loss: 3.9416\n",
      "Epoch 289/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4853 - val_loss: 3.9085\n",
      "Epoch 290/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4342 - val_loss: 3.9280\n",
      "Epoch 291/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.3555 - val_loss: 3.9068\n",
      "Epoch 292/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.5511 - val_loss: 3.9499\n",
      "Epoch 293/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4139 - val_loss: 3.9155\n",
      "Epoch 294/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.4659 - val_loss: 3.9410\n",
      "Epoch 295/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.5238 - val_loss: 3.9245\n",
      "Epoch 296/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4482 - val_loss: 3.9109\n",
      "Epoch 297/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4035 - val_loss: 3.8913\n",
      "Epoch 298/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4854 - val_loss: 3.9267\n",
      "Epoch 299/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.4492 - val_loss: 3.9045\n",
      "Epoch 300/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.4322 - val_loss: 3.9345\n",
      "Epoch 301/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.4530 - val_loss: 3.9297\n",
      "Epoch 302/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.4040 - val_loss: 3.9157\n",
      "Epoch 303/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.4028 - val_loss: 3.8973\n",
      "Epoch 304/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.4409 - val_loss: 3.8989\n",
      "Epoch 305/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.3725 - val_loss: 3.9073\n",
      "Epoch 306/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.3328 - val_loss: 3.8903\n",
      "Epoch 307/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.3610 - val_loss: 3.8985\n",
      "Epoch 308/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.4202 - val_loss: 3.8956\n",
      "Epoch 309/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.3938 - val_loss: 3.9265\n",
      "Epoch 310/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.3709 - val_loss: 3.9006\n",
      "Epoch 311/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4239 - val_loss: 3.9194\n",
      "Epoch 312/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.3753 - val_loss: 3.8954\n",
      "Epoch 313/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4243 - val_loss: 3.8990\n",
      "Epoch 314/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4538 - val_loss: 3.9489\n",
      "Epoch 315/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3737 - val_loss: 3.9203\n",
      "Epoch 316/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4103 - val_loss: 3.9039\n",
      "Epoch 317/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4029 - val_loss: 3.9076\n",
      "Epoch 318/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4314 - val_loss: 3.9329\n",
      "Epoch 319/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3964 - val_loss: 3.9275\n",
      "Epoch 320/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.4069 - val_loss: 3.9342\n",
      "Epoch 321/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3965 - val_loss: 3.9447\n",
      "Epoch 322/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3269 - val_loss: 3.8952\n",
      "Epoch 323/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3280 - val_loss: 3.8883\n",
      "Epoch 324/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3719 - val_loss: 3.8997\n",
      "Epoch 325/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.4085 - val_loss: 3.9031\n",
      "Epoch 326/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3890 - val_loss: 3.8956\n",
      "Epoch 327/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3479 - val_loss: 3.9092\n",
      "Epoch 328/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.3383 - val_loss: 3.8974\n",
      "Epoch 329/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.3844 - val_loss: 3.8993\n",
      "Epoch 330/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.3272 - val_loss: 3.9254\n",
      "Epoch 331/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.4151 - val_loss: 3.9200\n",
      "Epoch 332/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.3721 - val_loss: 3.9610\n",
      "Epoch 333/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.3526 - val_loss: 3.9069\n",
      "Epoch 334/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.3589 - val_loss: 3.9206\n",
      "Epoch 335/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.3597 - val_loss: 3.9239\n",
      "Epoch 336/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3658 - val_loss: 3.9093\n",
      "Epoch 337/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.4176 - val_loss: 3.9038\n",
      "Epoch 338/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.4635 - val_loss: 3.9316\n",
      "Epoch 339/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.3587 - val_loss: 3.9161\n",
      "Epoch 340/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.3726 - val_loss: 4.0283\n",
      "Epoch 341/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4746 - val_loss: 3.9100\n",
      "Epoch 342/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3417 - val_loss: 3.8994\n",
      "Epoch 343/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.3754 - val_loss: 3.9183\n",
      "Epoch 344/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.2873 - val_loss: 3.8950\n",
      "Epoch 345/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3917 - val_loss: 3.9130\n",
      "Epoch 346/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.4485 - val_loss: 3.9114\n",
      "Epoch 347/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3389 - val_loss: 3.9210\n",
      "Epoch 348/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3019 - val_loss: 3.9173\n",
      "Epoch 349/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.3382 - val_loss: 3.9336\n",
      "Epoch 350/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.3946 - val_loss: 3.9020\n",
      "Epoch 351/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.3167 - val_loss: 3.8879\n",
      "Epoch 352/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3806 - val_loss: 3.8844\n",
      "Epoch 353/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3554 - val_loss: 3.9053\n",
      "Epoch 354/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.3518 - val_loss: 3.9326\n",
      "Epoch 355/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.2940 - val_loss: 3.9558\n",
      "Epoch 356/500\n",
      "3007/3007 [==============================] - 0s 48us/step - loss: 4.3419 - val_loss: 3.9165\n",
      "Epoch 357/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.3866 - val_loss: 3.9363\n",
      "Epoch 358/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.3336 - val_loss: 3.8976\n",
      "Epoch 359/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.3410 - val_loss: 3.9180\n",
      "Epoch 360/500\n",
      "3007/3007 [==============================] - 0s 46us/step - loss: 4.3238 - val_loss: 3.9687\n",
      "Epoch 361/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.2944 - val_loss: 3.9122\n",
      "Epoch 362/500\n",
      "3007/3007 [==============================] - 0s 47us/step - loss: 4.3321 - val_loss: 3.9073\n",
      "Epoch 363/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3919 - val_loss: 3.9023\n",
      "Epoch 364/500\n",
      "3007/3007 [==============================] - 0s 49us/step - loss: 4.2798 - val_loss: 3.9156\n",
      "Epoch 365/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3089 - val_loss: 3.9001\n",
      "Epoch 366/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.2980 - val_loss: 3.9321\n",
      "Epoch 367/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.2867 - val_loss: 3.9036\n",
      "Epoch 368/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.3085 - val_loss: 3.8962\n",
      "Epoch 369/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.3732 - val_loss: 3.9482\n",
      "Epoch 370/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3215 - val_loss: 3.9347\n",
      "Epoch 371/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3801 - val_loss: 3.9318\n",
      "Epoch 372/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3450 - val_loss: 3.9017\n",
      "Epoch 373/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3533 - val_loss: 3.9124\n",
      "Epoch 374/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3553 - val_loss: 3.9240\n",
      "Epoch 375/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.2724 - val_loss: 3.9175\n",
      "Epoch 376/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3114 - val_loss: 3.9118\n",
      "Epoch 377/500\n",
      "3007/3007 [==============================] - 0s 43us/step - loss: 4.3393 - val_loss: 3.9512\n",
      "Epoch 378/500\n",
      "3007/3007 [==============================] - 0s 42us/step - loss: 4.3063 - val_loss: 3.9192\n",
      "Epoch 379/500\n",
      "3007/3007 [==============================] - 0s 45us/step - loss: 4.4096 - val_loss: 3.9208\n",
      "Epoch 380/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.2721 - val_loss: 3.9149\n",
      "Epoch 381/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3073 - val_loss: 3.9210\n",
      "Epoch 382/500\n",
      "3007/3007 [==============================] - 0s 44us/step - loss: 4.3080 - val_loss: 3.9249\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  train_X, train_y,\n",
    "  epochs=500, validation_split = 0.2, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.976194893651008"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model.evaluate(test_X, test_y, verbose=2)\n",
    "np.sqrt(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e4453a63d11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'MAE: {mae}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'RMSE: {rmse}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0;32m--> 220\u001b[0;31m                                      \"{!r}\".format(__name__, attr))\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'root'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "preds = model.predict(test_X)\n",
    "mae = mean_absolute_error(test_y, preds)\n",
    "rmse = np.sqrt(mae)\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
